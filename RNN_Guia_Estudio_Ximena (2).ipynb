{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFL2jpg5ZxPw"
      },
      "source": [
        "# Guía de Estudio: Redes Neuronales Recurrentes (RNN) con TensorFlow y Keras\n",
        "**Autora: Ximena Carolina Fernandez Cardenas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsSnbif8ZxPx"
      },
      "source": [
        "## 1. Introducción a las Redes Neuronales Recurrentes (RNN)\n",
        "\n",
        "Las **Redes Neuronales Recurrentes (RNN)** son un tipo de red diseñada para trabajar con datos secuenciales, como texto, series temporales, audio, etc. Su característica principal es que pueden **mantener memoria del estado anterior** mediante conexiones recurrentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP8Do0ZrZxPy"
      },
      "source": [
        "## 2. Características clave\n",
        "\n",
        "- Manejan secuencias de longitud variable.\n",
        "- Mantienen un estado interno que captura dependencias temporales.\n",
        "- Pueden usarse para tareas de clasificación, predicción de secuencia, generación de texto, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfhXjFD2ZxPy"
      },
      "source": [
        "## 3. Arquitecturas relacionadas\n",
        "\n",
        "- **Simple RNN**: estructura básica, propensa al desvanecimiento del gradiente.\n",
        "- **LSTM**: memoria a largo plazo, evita el problema de gradientes.\n",
        "- **GRU**: similar a LSTM pero con una arquitectura más simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I5kynYRWZxPy"
      },
      "outputs": [],
      "source": [
        "# Paso 1: Importar librerías\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZItr0370ZxPz"
      },
      "outputs": [],
      "source": [
        "# Paso 1: Cargar y preprocesar el dataset (Shakespeare)\n",
        "dataset, info = tfds.load(\"tiny_shakespeare\", with_info=True)\n",
        "\n",
        "text_data = \"\"\n",
        "for example in tfds.as_numpy(dataset['train']):\n",
        "    # 'example' es un diccionario con clave 'text'\n",
        "    text_data += example['text'].decode('utf-8')\n",
        "\n",
        "# Crear vocabulario y mapeos\n",
        "vocab = sorted(set(text_data))\n",
        "char2idx = {char: i for i, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IhYiSWHWZxPz"
      },
      "outputs": [],
      "source": [
        "# Paso 2: Preparar el conjunto de datos para entrenamiento\n",
        "seq_length = 100\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Parámetros de entrenamiento\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rWs_jk6jZxPz"
      },
      "outputs": [],
      "source": [
        "# Paso 3: Construir el modelo RNN\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "        ),\n",
        "        tf.keras.layers.SimpleRNN(\n",
        "            units=rnn_units,\n",
        "            return_sequences=True,\n",
        "            recurrent_initializer='glorot_uniform'\n",
        "        ),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MBPa-042ZxP0"
      },
      "outputs": [],
      "source": [
        "# Paso 4: Función de pérdida y compilación\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_mbGWygZxP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f8fa81-0c57-422c-8b4d-182cd1586c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 2s/step - loss: 3.0610\n",
            "Epoch 2/10\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 2s/step - loss: 2.0898\n",
            "Epoch 3/10\n",
            "\u001b[1m122/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 2s/step - loss: 1.8614"
          ]
        }
      ],
      "source": [
        "# Paso 5: Entrenamiento\n",
        "EPOCHS = 10\n",
        "model.fit(dataset, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DMm69ijZxP0"
      },
      "outputs": [],
      "source": [
        "# Paso 6: Generación de texto\n",
        "def generate_text(model, start_string, num_generate=100):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    temperature = 1.0\n",
        "\n",
        "    model.reset_states()\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "print(\"\\n--- Texto generado ---\\n\")\n",
        "print(generate_text(model, start_string=\"To be, or not to be: \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Explicación de funciones y parámetros\n",
        "\n",
        "### `Embedding`:\n",
        "Capa que convierte cada carácter (índice entero) en un vector denso.\n",
        "\n",
        "- `input_dim=vocab_size`: tamaño del vocabulario.\n",
        "- `output_dim=embedding_dim`: dimensión del vector de salida.\n",
        "\n",
        "### `SimpleRNN`:\n",
        "- `units=rnn_units`: número de neuronas en la capa RNN.\n",
        "- `return_sequences=True`: devuelve la secuencia completa (necesario para múltiples capas o salida secuencial).\n",
        "- `stateful=True`: mantiene estado entre lotes, útil para secuencias largas o texto continuo.\n",
        "\n",
        "### `Dense`:\n",
        "Capa de salida que proyecta hacia `vocab_size` (una predicción por carácter).\n",
        "\n",
        "---\n",
        "\n",
        "### `loss`:\n",
        "Se utiliza entropía cruzada categórica con `from_logits=True` porque la salida de la red **no pasa por softmax** (la capa `Dense` genera logits crudos).\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusiones\n",
        "\n",
        "- Las **RNN** son poderosas para trabajar con secuencias, pero tienen limitaciones (como el desvanecimiento del gradiente) que las arquitecturas **LSTM** y **GRU** resuelven mejor.\n",
        "- **Keras** permite construir RNNs con pocas líneas de código.\n",
        "- Este pipeline es útil para **generación de texto**, **análisis de sentimientos**, predicción de series temporales, entre otros casos.\n"
      ],
      "metadata": {
        "id": "qB-YxR_kauQt"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}